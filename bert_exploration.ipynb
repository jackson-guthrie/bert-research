{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Using Google BERT for Text Classification on Wikipedia Data**\n",
    "#### Author: Jackson Guthrie\n",
    "#### Last Updated: May 2024\n",
    "\n",
    "##### The goal of this notebook is for me to try to use a publicly-available dataset to implement Google BERT for a text multiclassification problem with high cardinality. This Wikipedia dataset contains a column with 90 classes, which should be sufficient for high cardinality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/17577/code/kaggle_input\\DBPEDIA_test.csv\n",
      "C:/Users/17577/code/kaggle_input\\DBPEDIA_train.csv\n",
      "C:/Users/17577/code/kaggle_input\\DBPEDIA_val.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "for dirname, _, filenames in os.walk('C:/Users/17577/code/kaggle_input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Struggling to get Tensorflow to be able to leverage GPU on Windows 10. Might retry on Mac or Linux\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Data Into Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Train:(240942, 4)\n",
      "Shape of Test:(60794, 4)\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "train_path = 'C:/Users/17577/code/kaggle_input\\DBPEDIA_train.csv'\n",
    "df = pd.read_csv(train_path)\n",
    "df.rename(columns = {'l1':'class', 'l2':'profession', 'l3':'type'}, inplace = True)\n",
    "\n",
    "# Load testing data\n",
    "test_path = 'C:/Users/17577/code/kaggle_input\\DBPEDIA_test.csv'\n",
    "test = pd.read_csv(test_path)\n",
    "test.rename(columns = {'l1':'class', 'l2':'profession', 'l3':'type'}, inplace = True)\n",
    "\n",
    "# Size datasets\n",
    "print(\"Shape of Train:{}\\nShape of Test:{}\".format(df.shape, test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA on the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>profession</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>William Alexander Massey (October 7, 1856 – Ma...</td>\n",
       "      <td>Agent</td>\n",
       "      <td>Politician</td>\n",
       "      <td>Senator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lions is the sixth studio album by American ro...</td>\n",
       "      <td>Work</td>\n",
       "      <td>MusicalWork</td>\n",
       "      <td>Album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pirqa (Aymara and Quechua for wall, hispaniciz...</td>\n",
       "      <td>Place</td>\n",
       "      <td>NaturalPlace</td>\n",
       "      <td>Mountain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cancer Prevention Research is a biweekly peer-...</td>\n",
       "      <td>Work</td>\n",
       "      <td>PeriodicalLiterature</td>\n",
       "      <td>AcademicJournal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Princeton University Chapel is located on ...</td>\n",
       "      <td>Place</td>\n",
       "      <td>Building</td>\n",
       "      <td>HistoricBuilding</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  class  \\\n",
       "0  William Alexander Massey (October 7, 1856 – Ma...  Agent   \n",
       "1  Lions is the sixth studio album by American ro...   Work   \n",
       "2  Pirqa (Aymara and Quechua for wall, hispaniciz...  Place   \n",
       "3  Cancer Prevention Research is a biweekly peer-...   Work   \n",
       "4  The Princeton University Chapel is located on ...  Place   \n",
       "\n",
       "             profession              type  \n",
       "0            Politician           Senator  \n",
       "1           MusicalWork             Album  \n",
       "2          NaturalPlace          Mountain  \n",
       "3  PeriodicalLiterature   AcademicJournal  \n",
       "4              Building  HistoricBuilding  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "profession\n",
       "Athlete             31111\n",
       "Person              19504\n",
       "Animal              14682\n",
       "Building            10704\n",
       "Politician           9504\n",
       "                    ...  \n",
       "MusicalArtist         198\n",
       "RaceTrack             172\n",
       "ComicsCharacter       144\n",
       "VolleyballPlayer      137\n",
       "Database              129\n",
       "Name: count, Length: 70, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['profession'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We will need to adjust our preprocessing methods because the classes are not uniformly distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoding the Professions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Politician': 0,\n",
       " 'MusicalWork': 1,\n",
       " 'NaturalPlace': 2,\n",
       " 'PeriodicalLiterature': 3,\n",
       " 'Building': 4,\n",
       " 'Animal': 5,\n",
       " 'Organisation': 6,\n",
       " 'Person': 7,\n",
       " 'Athlete': 8,\n",
       " 'Settlement': 9,\n",
       " 'LegalCase': 10,\n",
       " 'MotorcycleRider': 11,\n",
       " 'Company': 12,\n",
       " 'RouteOfTransportation': 13,\n",
       " 'SocietalEvent': 14,\n",
       " 'WinterSportPlayer': 15,\n",
       " 'ClericalAdministrativeRegion': 16,\n",
       " 'EducationalInstitution': 17,\n",
       " 'BodyOfWater': 18,\n",
       " 'Plant': 19,\n",
       " 'Infrastructure': 20,\n",
       " 'FootballLeagueSeason': 21,\n",
       " 'Actor': 22,\n",
       " 'SportsManager': 23,\n",
       " 'Cleric': 24,\n",
       " 'Boxer': 25,\n",
       " 'Cartoon': 26,\n",
       " 'Venue': 27,\n",
       " 'Artist': 28,\n",
       " 'Tournament': 29,\n",
       " 'Coach': 30,\n",
       " 'ComicsCharacter': 31,\n",
       " 'Olympics': 32,\n",
       " 'SportsTeamSeason': 33,\n",
       " 'Software': 34,\n",
       " 'Group': 35,\n",
       " 'Broadcaster': 36,\n",
       " 'Tower': 37,\n",
       " 'Race': 38,\n",
       " 'SportFacility': 39,\n",
       " 'SportsTeam': 40,\n",
       " 'SportsEvent': 41,\n",
       " 'Eukaryote': 42,\n",
       " 'Scientist': 43,\n",
       " 'CelestialBody': 44,\n",
       " 'Engine': 45,\n",
       " 'BritishRoyalty': 46,\n",
       " 'Satellite': 47,\n",
       " 'Comic': 48,\n",
       " 'WrittenWork': 49,\n",
       " 'FictionalCharacter': 50,\n",
       " 'Presenter': 51,\n",
       " 'Horse': 52,\n",
       " 'NaturalEvent': 53,\n",
       " 'AmusementParkAttraction': 54,\n",
       " 'Writer': 55,\n",
       " 'Song': 56,\n",
       " 'RaceTrack': 57,\n",
       " 'SportsLeague': 58,\n",
       " 'Genre': 59,\n",
       " 'GridironFootballPlayer': 60,\n",
       " 'Database': 61,\n",
       " 'RacingDriver': 62,\n",
       " 'Wrestler': 63,\n",
       " 'Stream': 64,\n",
       " 'FloweringPlant': 65,\n",
       " 'Station': 66,\n",
       " 'OrganisationMember': 67,\n",
       " 'MusicalArtist': 68,\n",
       " 'VolleyballPlayer': 69}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_labels = df.profession.unique()\n",
    "\n",
    "label_dict = {}\n",
    "for index, possible_label in enumerate(possible_labels):\n",
    "    label_dict[possible_label] = index\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17577\\AppData\\Local\\Temp\\ipykernel_24976\\3741609252.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['label'] = df.profession.replace(label_dict)\n"
     ]
    }
   ],
   "source": [
    "# Add enumerated label to data\n",
    "df['label'] = df.profession.replace(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>profession</th>\n",
       "      <th>label</th>\n",
       "      <th>data_split</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Actor</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">22</th>\n",
       "      <th>train</th>\n",
       "      <td>943</td>\n",
       "      <td>943</td>\n",
       "      <td>943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>236</td>\n",
       "      <td>236</td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">AmusementParkAttraction</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">54</th>\n",
       "      <th>train</th>\n",
       "      <td>385</td>\n",
       "      <td>385</td>\n",
       "      <td>385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Animal</th>\n",
       "      <th>5</th>\n",
       "      <th>train</th>\n",
       "      <td>11745</td>\n",
       "      <td>11745</td>\n",
       "      <td>11745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wrestler</th>\n",
       "      <th>63</th>\n",
       "      <th>val</th>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Writer</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">55</th>\n",
       "      <th>train</th>\n",
       "      <td>860</td>\n",
       "      <td>860</td>\n",
       "      <td>860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>215</td>\n",
       "      <td>215</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">WrittenWork</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">49</th>\n",
       "      <th>train</th>\n",
       "      <td>1232</td>\n",
       "      <td>1232</td>\n",
       "      <td>1232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>308</td>\n",
       "      <td>308</td>\n",
       "      <td>308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           text  class   type\n",
       "profession              label data_split                     \n",
       "Actor                   22    train         943    943    943\n",
       "                              val           236    236    236\n",
       "AmusementParkAttraction 54    train         385    385    385\n",
       "                              val            96     96     96\n",
       "Animal                  5     train       11745  11745  11745\n",
       "...                                         ...    ...    ...\n",
       "Wrestler                63    val            61     61     61\n",
       "Writer                  55    train         860    860    860\n",
       "                              val           215    215    215\n",
       "WrittenWork             49    train        1232   1232   1232\n",
       "                              val           308    308    308\n",
       "\n",
       "[140 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data, stratified by the label since the labels are not balanced. \n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    df.index.values,\n",
    "    df.label.values,\n",
    "    test_size = 0.2,\n",
    "    random_state = 24,\n",
    "    stratify = df.label.values\n",
    ")\n",
    "\n",
    "df['data_split'] = ['not_set'] * df.shape[0]\n",
    "df.loc[x_train, 'data_split'] = 'train'\n",
    "df.loc[x_val, 'data_split'] = 'val'\n",
    "df.groupby(['profession', 'label', 'data_split']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Have TensorFlow and BERT Tokenize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
     ]
    }
   ],
   "source": [
    "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8' \n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessing model\n",
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Data\n",
    "text_preprocessed = bert_preprocess_model(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT Model\n",
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Exception encountered when calling layer \"keras_layer_1\" \"                 f\"(type KerasLayer).\n\nGraph execution error:\n\nOOM when allocating tensor with shape[30840576,512] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node word_embeddings/Gather}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_restored_function_body_27828]\n\nCall arguments received by layer \"keras_layer_1\" \"                 f\"(type KerasLayer):\n  • inputs={'input_mask': 'tf.Tensor(shape=(240942, 128), dtype=int32)', 'input_type_ids': 'tf.Tensor(shape=(240942, 128), dtype=int32)', 'input_word_ids': 'tf.Tensor(shape=(240942, 128), dtype=int32)'}\n  • training=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Encode Text\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m bert_results \u001b[38;5;241m=\u001b[39m \u001b[43mbert_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_preprocessed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoaded BERT: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtfhub_handle_encoder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPooled Outputs Shape:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbert_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpooled_output\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\17577\\.conda\\envs\\bertEnv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\17577\\.conda\\envs\\bertEnv\\lib\\site-packages\\tensorflow_hub\\keras_layer.py:250\u001b[0m, in \u001b[0;36mKerasLayer.call\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m    247\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;66;03m# Behave like BatchNormalization. (Dropout is different, b/181839368.)\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 250\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43msmart_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msmart_cond\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m                                 \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m                                 \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;66;03m# Unwrap dicts returned by signatures.\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_key:\n",
      "File \u001b[1;32mc:\\Users\\17577\\.conda\\envs\\bertEnv\\lib\\site-packages\\tensorflow_hub\\keras_layer.py:252\u001b[0m, in \u001b[0;36mKerasLayer.call.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    247\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;66;03m# Behave like BatchNormalization. (Dropout is different, b/181839368.)\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    250\u001b[0m   result \u001b[38;5;241m=\u001b[39m smart_cond\u001b[38;5;241m.\u001b[39msmart_cond(training,\n\u001b[0;32m    251\u001b[0m                                  \u001b[38;5;28;01mlambda\u001b[39;00m: f(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m--> 252\u001b[0m                                  \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[0;32m    254\u001b[0m \u001b[38;5;66;03m# Unwrap dicts returned by signatures.\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_key:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Exception encountered when calling layer \"keras_layer_1\" \"                 f\"(type KerasLayer).\n\nGraph execution error:\n\nOOM when allocating tensor with shape[30840576,512] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node word_embeddings/Gather}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_restored_function_body_27828]\n\nCall arguments received by layer \"keras_layer_1\" \"                 f\"(type KerasLayer):\n  • inputs={'input_mask': 'tf.Tensor(shape=(240942, 128), dtype=int32)', 'input_type_ids': 'tf.Tensor(shape=(240942, 128), dtype=int32)', 'input_word_ids': 'tf.Tensor(shape=(240942, 128), dtype=int32)'}\n  • training=None"
     ]
    }
   ],
   "source": [
    "# Encode Text\n",
    "bert_results = bert_model(text_preprocessed)\n",
    "\n",
    "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
    "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
    "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
    "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
    "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
